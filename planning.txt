Problem Statement:
We'd like you to write a simple web crawler in a programming language you're familiar with.
Given a starting URL, the crawler should visit each URL it finds on the same domain.
It should print each URL visited, and a list of links found on that page.
The crawler should be limited to one subdomain - so when you start with *https://monzo.com/*,
it would crawl all pages on the monzo.com website, but not follow external links,
for example to facebook.com or community.monzo.com.

Ideally, write it as you would a production piece of code.
This exercise is not meant to show us whether you can write code â€“
we are more interested in how you design software.
This means that we care less about a fancy UI or sitemap format,
and more about how your program is structured: the trade-offs you've made,
what behaviour the program exhibits, and your use of concurrency, test coverage, and so on.

Edge cases:
1. The crawler should work only for the given sub-domain of the page.
    a. If say base URL is https://abc.website.com/contact-us, then we should only print the pages for https://abc.website.com. Not any other website,
    Even domains like website.com will be rejected as it will not match the provided base URL.
2. Remove Duplicates
3. Ignore all the other sub-domains than the given one.
4. Handle cases where given URL is:
    a. is null
    b. Incorrect URL
    c. blank URL
    d. Valid URL but domain is unreachable.
    e. Just one page is resulting in some HTTP Error.
5. Avoid calling the same page for avoiding infinite loops.
6. IN Anchor Tag, ingore the href's pointing to the ID's of the same page. Because ideally the page will remain the same.
7. Print, images if they are on the same domain, otherwise ignore. Usually, images are on CDN but still an edge case.
8. How to handle relative paths.
    a. What if the base url is "https://abc.com/a/b/c" and one the URL on the page is "../d" which is equal to
    https://abc.com/a/b/d
    b. First check if relative path will result in correct URL.
    c.



Psuedo:
1. Fetch All links from the page.
2. Pass them to a Kafka Topic. say find-other-links(For crawling it further) and final-output(the final output stored in a topic).
3. Once received, filter the messages as per the given logic of keeping the same domain. This
is very important because if you decide in future to allow some other logic, you just need to change the consumer
side while the overall application is working fine. And even you plan to make the entire consumer side
down.
4. Find the links on the received link.
5. Remove Duplicates.
6. Correct the base domains so that the URL can be used to download the html page.
7. Follow the above activity again.

Note: We might want to store the links received in some kind of DB.
Always make sure to maintain the source page together with the link so that it becomes easier to check the output.

Planning:
1. Use Jsoup Library in Java for HTML parsing.
2. Read about various ways to add link to a page.
    a. Anchor tag,
    b. CSS
    c. JavaScript.
3. Figure out how to find domains as well as subdomains from the given URL in Java.


Basic:
1. Validate the URL, and error in case invalid URL.
2. If valid, Find All LINK tags as well as Anchor tags on the given URL.
3. Strip, Fragment Identifiers from the URL Example: http://www.cnn.com/news/headlines.html#SPORTS
4. Trim URL's or spaces.
5. Re-Construct all the URL's such that the domain is made to lowercase and rest of the URL remains the same.
6. Convert Absolute URL's to Absolute URL's
7. Filter only Unique.
8. Filter URL's belonging to the same domain as in the Given URL.
9. Now repeat the same process.

Assumptions:
1. Considering the URL's are case sensitives.
2. Currently there is no way to keep a list of already visited URL's. This will become super critical to avoid
loops.



